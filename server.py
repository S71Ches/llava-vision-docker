import os
import base64

from fastapi import FastAPI, UploadFile, File
from llama_cpp import Llama

# 1) –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —á–µ—Ä–µ–∑ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
MODEL_PATH = os.getenv("MODEL_PATH", "/models/llava-v1.6-mistral-7b.Q6_K.gguf")
CHAT_FORMAT = "llava-1-6"
N_GPU_LAYERS = int(os.getenv("GPU_LAYERS", "16"))
F16_KV = True

# 2) –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
llm = Llama(
    model_path=MODEL_PATH,
    chat_format=CHAT_FORMAT,
    n_gpu_layers=N_GPU_LAYERS,
    f16_kv=F16_KV,
)

app = FastAPI(title="Vision Captioning Service")

# 3) Health-check
@app.get("/")
def health():
    return {"message": "üöÄ Vision model loaded and ready!"}

# 4) –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è captioning
@app.post("/vision/caption")
async def caption(file: UploadFile = File(...)):
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É, –∫–æ–¥–∏—Ä—É–µ–º –µ—ë –≤ base64,
    –ø–æ–¥—Å–æ–≤—ã–≤–∞–µ–º LLaVA-–ø—Ä–æ–º–ø—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç.
    """
    # —á–∏—Ç–∞–µ–º –±–∞–π—Ç—ã –∫–∞—Ä—Ç–∏–Ω–∫–∏
    data = await file.read()
    # base64
    b64 = base64.b64encode(data).decode("utf-8")
    # —Ñ–æ—Ä–º–∏—Ä—É–µ–º prompt –¥–ª—è LLaVA
    prompt = f"<img>{b64}</img>\nDescribe the image in one sentence:"
    # –≤—ã–∑—ã–≤–∞–µ–º –º–æ–¥–µ–ª—å
    resp = llm(prompt=prompt, max_tokens=128, temperature=0.2)
    text = resp["choices"][0]["text"].strip()
    return {"caption": text}
